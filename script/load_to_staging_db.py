import mysql.connector
import os
import sys
import argparse
from pathlib import Path
from mysql.connector import Error
from datetime import datetime, timedelta

# Import c√°c module ti·ªán √≠ch
from load_config import load_config
from log_manager import log_process_action, log_conf_action, get_process_log_value
from param_sync import get_parameter_value
from send_mail import send_email

# --- C·∫§U H√åNH PROCESS ---
PROCESS_NAME = "load_to_staging"
PREV_PROCESS = "crawling" # T√™n process tr∆∞·ªõc ƒë√≥ ƒë·ªÉ check log
PROCESS_ID = 2 
SEND_TO_EMAIL = get_parameter_value('SEND_TO_EMAIL')

# Load Config DB
config = load_config()
STAGING_CONFIG = config["DB_CONFIGS"]['STAGING']
# B·∫Øt bu·ªôc b·∫≠t local_infile cho client python
STAGING_CONFIG['allow_local_infile'] = True

def execute_load_data(csv_file_path):
    """
    H√†m logic ch√≠nh: Th·ª±c thi k·∫øt n·ªëi DB v√† load file
    """
    csv_path_obj = Path(csv_file_path).resolve()
    
    if not csv_path_obj.exists():
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file CSV: {csv_path_obj}")

    conn = None
    cursor = None
    
    try:
        conn = mysql.connector.connect(**STAGING_CONFIG)
        cursor = conn.cursor()
        
        # 1. TRUNCATE b·∫£ng staging c≈©
        print("   -> Cleaning old data (TRUNCATE stg_products)...")
        cursor.execute("TRUNCATE TABLE stg_products;")
        
        # 2. LOAD DATA INFILE
        # L∆∞u √Ω: ƒê∆∞·ªùng d·∫´n file ph·∫£i l√† ki·ªÉu Unix (/) ngay c·∫£ tr√™n Windows
        sql_path = str(csv_path_obj).replace('\\', '/')
        
        print(f"   -> Loading file: {sql_path}")
        
        load_query = f"""
        LOAD DATA LOCAL INFILE '{sql_path}'
        INTO TABLE stg_products
        FIELDS TERMINATED BY ',' 
        ENCLOSED BY '"'
        LINES TERMINATED BY '\\n'
        IGNORE 1 ROWS
        (name, province, date, price) 
        SET load_date = NOW();
        """
        
        # C·∫ßn set global local_infile = 1 (n·∫øu server ch∆∞a b·∫≠t)
        cursor.execute("SET GLOBAL local_infile = 1;")
        cursor.execute(load_query)
        
        records_loaded = cursor.rowcount
        conn.commit()
        
        print(f"   -> Success! Loaded {records_loaded} rows.")
        return records_loaded

    except Error as e:
        print(f"   -> MySQL Error: {e}")
        raise e
    finally:
        if cursor: cursor.close()
        if conn and conn.is_connected(): conn.close()

def run_load_staging(target_date_str=None, force_run=False):
    """
    H√†m ƒëi·ªÅu ph·ªëi: Ki·ªÉm tra log Crawl -> T√≠nh t√™n file -> G·ªçi h√†m Load
    """
    start_time = datetime.now()
    
    # 1. X√ÅC ƒê·ªäNH NG√ÄY M·ªêC
    file_target_date = None
    if target_date_str:
        allowed_formats = ['%Y-%m-%d', '%d/%m/%Y', '%Y/%m/%d', '%d-%m-%Y']
        for fmt in allowed_formats:
            try:
                file_target_date = datetime.strptime(target_date_str, fmt)
                break 
            except ValueError:
                continue 
        
        if file_target_date is None:
            print(f"‚ùå L·ªói ƒë·ªãnh d·∫°ng ng√†y '{target_date_str}'! Vui l√≤ng nh·∫≠p dd/mm/yyyy ho·∫∑c YYYY-MM-DD")
            return None
    else:
        file_target_date = datetime.now()

    print(f"--- B·∫ÆT ƒê·∫¶U LOAD TO STAGING (Target Date: {file_target_date.strftime('%d/%m/%Y')}, Force={force_run}) ---")

    # 2. KI·ªÇM TRA DEPENDENCY (LOG CHECK)
    if not force_run:
        check_log_date = datetime.now().strftime('%Y-%m-%d')
        print(f"üîç Ki·ªÉm tra log b∆∞·ªõc '{PREV_PROCESS}' ng√†y ch·∫°y {check_log_date}...")
        
        prev_status = get_process_log_value(PREV_PROCESS, check_log_date)
        
        # Backup case: t√¨m log ng√†y target
        if prev_status == "Null" or prev_status is None:
            backup_log_date = file_target_date.strftime('%Y-%m-%d')
            if backup_log_date != check_log_date:
                print(f"‚ö†Ô∏è Kh√¥ng th·∫•y log h√¥m nay. Th·ª≠ t√¨m log ng√†y target: {backup_log_date}...")
                prev_status = get_process_log_value(PREV_PROCESS, backup_log_date)

        # X·ª≠ l√Ω tr·∫°ng th√°i
        if prev_status == "CND":
            msg = f"‚ö†Ô∏è B∆∞·ªõc {PREV_PROCESS} b√°o 'No Data'. B·ªè qua b∆∞·ªõc Load."
            print(msg)
            # Log tr·∫°ng th√°i Skip
            log_process_action(PROCESS_ID, PROCESS_NAME, start_time, datetime.now(), "LS_SKIP", 0, 0, 0, "Skipped: No Data form Crawler")
            return None

        if prev_status != "CS" and prev_status != "LS": # CS: Completed Success (Crawler)
            msg = f"‚ùå Kh√¥ng th·ªÉ ch·∫°y Load v√¨ {PREV_PROCESS} ch∆∞a th√†nh c√¥ng (Status: {prev_status}). D√πng --force ƒë·ªÉ b·ªè qua."
            print(msg)
            return None
        
        print(f"‚úÖ B∆∞·ªõc {PREV_PROCESS} OK (Status: {prev_status}).")
    else:
        print(f"‚ö†Ô∏è FORCE MODE: B·ªè qua ki·ªÉm tra log c·ªßa {PREV_PROCESS}.")

    # 3. T√çNH TO√ÅN T√äN FILE (Logic: Crawler l∆∞u t√™n file theo kho·∫£ng th·ªùi gian)
    # Gi·∫£ ƒë·ªãnh crawler ch·∫°y cho kho·∫£ng 7 ng√†y k·∫øt th√∫c v√†o target_date
    start_date = file_target_date - timedelta(days=7)
    s_str = start_date.strftime('%d-%m-%Y')
    e_str = file_target_date.strftime('%d-%m-%Y')
    
    # L·∫•y ƒë∆∞·ªùng d·∫´n staging t·ª´ DB ho·∫∑c m·∫∑c ƒë·ªãnh
    staging_dir = get_parameter_value('STAGING_DIR') or "./staging"
    file_name = f"nong_san_{s_str}_{e_str}.csv"
    csv_path = os.path.join(staging_dir, file_name)

    print(f"üìÇ T√¨m file m·ª•c ti√™u: {csv_path}")

    try:
        if not os.path.exists(csv_path):
            msg = f"Kh√¥ng t√¨m th·∫•y file {file_name} (D√π log tr∆∞·ªõc ƒë√≥ b√°o OK ho·∫∑c Force Run)"
            print(f"‚ùå {msg}")
            # Log Fail
            log_process_action(PROCESS_ID, PROCESS_NAME, start_time, datetime.now(), "LF", 0, 0, 0, msg)
            return None

        # 4. Log START
        log_process_action(
            process_config_id=PROCESS_ID,
            process_name=PROCESS_NAME,
            start_time=start_time,
            end_time=None,
            status="LR", # Running
            message=f"Loading {file_name}"
        )

        # 5. TH·ª∞C THI LOAD
        records_loaded = execute_load_data(csv_path)

        # 6. Log SUCCESS
        end_time = datetime.now()
        log_process_action(
            process_config_id=PROCESS_ID,
            process_name=PROCESS_NAME,
            start_time=start_time,
            end_time=end_time,
            status="LS", # Load Success
            records_loaded=records_loaded,
            message=f"Loaded: {file_name}"
        )
        
        if SEND_TO_EMAIL:
             send_email(f"[ETL] LOAD SUCCESS", f"Loaded {records_loaded} rows from {file_name}", [SEND_TO_EMAIL])

        return records_loaded

    except Exception as e:
        end_time = datetime.now()
        error_msg = f"Load Error: {str(e)}"
        print(f"‚ùå {error_msg}")
        
        log_process_action(
            process_config_id=PROCESS_ID,
            process_name=PROCESS_NAME,
            start_time=start_time,
            end_time=end_time,
            status="LF", # Load Failed
            message=error_msg
        )
        if SEND_TO_EMAIL:
            send_email(f"[ETL] LOAD FAILED", f"Error: {e}", [SEND_TO_EMAIL])
        
        # N√©m l·ªói ra ngo√†i ƒë·ªÉ Pipeline bi·∫øt l√† th·∫•t b·∫°i
        sys.exit(1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run load staging process manually.")
    parser.add_argument("--date", type=str, default=None, help="Format dd/mm/yyyy or YYYY-MM-DD")
    
    # --- [QUAN TR·ªåNG] TH√äM D√íNG N√ÄY ƒê·ªÇ NH·∫¨N FORCE ---
    parser.add_argument("--force", action="store_true", help="Force run ignoring previous logs")
    
    args = parser.parse_args()

    run_load_staging(target_date_str=args.date, force_run=args.force)
    
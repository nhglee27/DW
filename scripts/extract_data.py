import os
import time
import glob
import pandas as pd
import argparse
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select, WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from mysql.connector import Error
from datetime import datetime, timedelta

# Import c√°c module ti·ªán √≠ch
from config import DB_CONFIGS
from log_manager import log_process_action, log_conf_action
from param_sync import get_parameter_value
from send_mail import send_email
from load_config import load_config

# --- C·∫§U H√åNH PROCESS ---
PROCESS_NAME = "crawling"
PROCESS_ID = 1  # ID trong b·∫£ng process_config
SEND_TO_EMAIL = get_parameter_value('SEND_TO_EMAIL')
source_url = get_parameter_value('source_url')

def download_nong_san_html_to_csv(start_date: str, end_date: str, download_dir: str = "./staging"):
    """
    H√†m logic ch√≠nh: T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ thitruongnongsan.gov.vn
    Tr·∫£ v·ªÅ: (csv_path, record_count)
    """
    # 1. T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ
    os.makedirs(download_dir, exist_ok=True)

    # 2. C·∫•u h√¨nh Chrome (B·∫Øt bu·ªôc cho Docker)
    chrome_options = webdriver.ChromeOptions()
    chrome_options.binary_location = "/usr/bin/chromium"  # Quan tr·ªçng cho Docker
    
    prefs = {
        "download.default_directory": os.path.abspath(download_dir),
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "safebrowsing.enabled": True
    }
    chrome_options.add_experimental_option("prefs", prefs)
    
    # C√°c c·ªù b·∫Øt bu·ªôc khi ch·∫°y trong container
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")

    driver = webdriver.Chrome(options=chrome_options)
    wait = WebDriverWait(driver, 30) # TƒÉng th·ªùi gian ch·ªù l√™n 30s cho m·∫°ng ch·∫≠m

    try:
        print(f"üåê ƒêang truy c·∫≠p website... ({start_date} - {end_date})")
        # --- LOGIC C√ÄO D·ªÆ LI·ªÜU C·ª¶A B·∫†N ---
        driver.get(source_url)

        # Nh·∫≠p ng√†y
        date_from = wait.until(EC.presence_of_element_located((By.ID, "ctl00_maincontent_tu_ngay")))
        date_from.clear()
        date_from.send_keys(start_date)
        
        driver.find_element(By.ID, "ctl00_maincontent_den_ngay").clear()
        driver.find_element(By.ID, "ctl00_maincontent_den_ngay").send_keys(end_date)

        # Ch·ªçn ng√†nh h√†ng v√† nh√≥m s·∫£n ph·∫©m
        Select(driver.find_element(By.ID, "ctl00_maincontent_Ng√†nh_h√†ng")).select_by_visible_text("Rau, qu·∫£")
        time.sleep(2) # Sleep nh·∫π ƒë·ªÉ dropdown load d·ªØ li·ªáu ph·ª• thu·ªôc
        Select(driver.find_element(By.ID, "ctl00_maincontent_Nh√≥m_s·∫£n_ph·∫©m")).select_by_visible_text("Rau c·ªß qu·∫£")

        # Nh·∫•n n√∫t "Xem"
        xem_btn = wait.until(EC.element_to_be_clickable((By.ID, "ctl00_maincontent_Xem")))
        driver.execute_script("arguments[0].click();", xem_btn)

        # Ch·ªù b·∫£ng d·ªØ li·ªáu xu·∫•t hi·ªán
        wait.until(EC.presence_of_element_located((By.ID, "ctl00_maincontent_GridView1")))

        # Ki·ªÉm tra n√∫t "T·∫£i Excel"
        try:
            excel_btn = wait.until(
                EC.element_to_be_clickable((By.ID, "ctl00_maincontent_tai_excel")),
                message="Kh√¥ng t√¨m th·∫•y n√∫t t·∫£i Excel (C√≥ th·ªÉ kh√¥ng c√≥ d·ªØ li·ªáu)"
            )
        except Exception:
            print(f"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ho·∫∑c n√∫t t·∫£i kh√¥ng hi·ªán trong kho·∫£ng {start_date} - {end_date}.")
            return None, 0

        # X√≥a file c≈© (xls) trong th∆∞ m·ª•c ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n
        for f in glob.glob(os.path.join(download_dir, "*.xls")):
            os.remove(f)

        # Click t·∫£i Excel
        print("‚¨áÔ∏è ƒêang t·∫£i file Excel...")
        driver.execute_script("arguments[0].click();", excel_btn)

        # Ch·ªù file xu·∫•t hi·ªán (Loop check)
        html_path = None
        for _ in range(20):
            files = glob.glob(os.path.join(download_dir, "*xls"))
            if files:
                html_path = files[0]
                break
            time.sleep(1)

        if not html_path:
            raise Exception("ƒê√£ click t·∫£i nh∆∞ng kh√¥ng th·∫•y file v·ªÅ th∆∞ m·ª•c.")

        # --- X·ª¨ L√ù FILE (CONVERT TO CSV) ---
        print(f"üìÇ ƒê√£ t·∫£i: {html_path}. ƒêang chuy·ªÉn ƒë·ªïi sang CSV...")
        
        # ƒê·ªçc b·∫£ng HTML (File ƒëu√¥i .xls c·ªßa web n√†y th·ª±c ch·∫•t l√† HTML)
        dfs = pd.read_html(html_path)
        if not dfs:
            raise Exception("File t·∫£i v·ªÅ kh√¥ng ch·ª©a b·∫£ng d·ªØ li·ªáu n√†o.")
        df = dfs[0]

        # T·∫°o t√™n file CSV chu·∫©n
        safe_start = start_date.replace("/", "-")
        safe_end = end_date.replace("/", "-")
        csv_name = f"nong_san_{safe_start}_{safe_end}.csv"
        csv_path = os.path.join(download_dir, csv_name)

        # L∆∞u CSV
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        print(f"‚úÖ ƒê√£ l∆∞u CSV: {csv_path}")
        
        # D·ªçn d·∫πp file r√°c
        os.remove(html_path)
        
        return csv_path, len(df)

    except Exception as e:
        raise e # N√©m l·ªói ra ngo√†i ƒë·ªÉ h√†m run_crawling b·∫Øt v√† log
    finally:
        driver.quit()

import argparse
import sys
import os
from datetime import datetime, timedelta
# Gi·ªØ nguy√™n c√°c import kh√°c c·ªßa b·∫°n (log_manager, config, selenium...)

# ... (Gi·ªØ nguy√™n ph·∫ßn c·∫•u h√¨nh v√† h√†m download_nong_san_html_to_csv ·ªü tr√™n) ...

def run_crawling(target_date=None, force_run=False):
    """
    H√†m ƒëi·ªÅu ph·ªëi vi·ªác ch·∫°y Crawl:
    - target_date: Ng√†y m·ªëc (YYYY-MM-DD). N·∫øu None l·∫•y ng√†y hi·ªán t·∫°i.
    - force_run: N·∫øu True s·∫Ω b·ªè qua check log (n·∫øu c√≥ logic check log).
    """
    # [QUAN TR·ªåNG] Kh·ªüi t·∫°o start_time ngay ƒë·∫ßu h√†m
    start_time = datetime.now() 

    # 1. X·ª≠ l√Ω ng√†y th√°ng
    if target_date:
        try:
            # L∆∞u √Ω: ƒê·ªãnh d·∫°ng chu·∫©n l√† YYYY-MM-DD (V√≠ d·ª•: 2025-11-06)
            current_date = datetime.strptime(target_date, '%Y-%m-%d')
        except ValueError:
            print(f"‚ùå L·ªói ƒë·ªãnh d·∫°ng ng√†y: {target_date}. Vui l√≤ng d√πng ƒë·ªãnh d·∫°ng YYYY-MM-DD")
            return None
    else:
        current_date = datetime.now()

    # Gi·∫£ s·ª≠ logic c√†o l√† l·∫•y d·ªØ li·ªáu 7 ng√†y g·∫ßn nh·∫•t t√≠nh t·ª´ target_date
    end_date_str = current_date.strftime('%d/%m/%Y')
    start_date = current_date - timedelta(days=7)
    start_date_str = start_date.strftime('%d/%m/%Y')

    print(f"--- B·∫ÆT ƒê·∫¶U EXTRACT DATA (Force={force_run}) ---")
    print(f"üìÖ Range: {start_date_str} - {end_date_str}")

    try:
        print(f"[{PROCESS_NAME}] B·∫Øt ƒë·∫ßu ch·∫°y quy tr√¨nh...")
        
        # Log START
        log_process_action(
            process_config_id=PROCESS_ID,
            process_name=PROCESS_NAME,
            start_time=start_time,
            end_time=None,
            status="START",
            message=f"Range: {start_date_str}-{end_date_str}"
        )

        staging_dir = get_parameter_value('STAGING_DIR') or "./staging"
        
        # G·ªçi h√†m crawl (H√†m n√†y b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n)
        csv_path, record_count = download_nong_san_html_to_csv(start_date_str, end_date_str, staging_dir)

        end_time = datetime.now()
        
        if not csv_path:
            msg = f"No data found for range {start_date_str}-{end_date_str}"
            print(msg)
            # Log SUCCESS nh∆∞ng record = 0
            log_process_action(PROCESS_ID, PROCESS_NAME, start_time, end_time, "CND", 0, 0, 0, msg)
            return None

        # Log SUCCESS
        log_process_action(
            process_config_id=PROCESS_ID,
            process_name=PROCESS_NAME,
            start_time=start_time,
            end_time=end_time,
            status="CS",
            records_extract=record_count,
            message=f"Saved: {os.path.basename(csv_path)}"
        )
        
        print(f"‚úÖ Ho√†n th√†nh! File l∆∞u t·∫°i: {csv_path}")
        return csv_path

    except Exception as e:
        end_time = datetime.now()
        error_msg = f"Crawler Error: {str(e)}"
        print(f"‚ùå {error_msg}")
        
        log_process_action(
            process_config_id=PROCESS_ID,
            process_name=PROCESS_NAME,
            start_time=start_time,
            end_time=end_time,
            status="CF",
            message=error_msg
        )
        
        # G·ª≠i mail n·∫øu c√≥ c·∫•u h√¨nh
        if 'SEND_TO_EMAIL' in globals() and SEND_TO_EMAIL:
             send_email(f"[ETL] CRAWLING FAILED", f"Error: {e}", [SEND_TO_EMAIL])
        
        raise e

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run extract process manually.")
    parser.add_argument("--date", type=str, default=None, help="Format YYYY-MM-DD (e.g., 2025-11-23)")
    parser.add_argument("--force", action="store_true", help="Force run ignoring logs")
    
    args = parser.parse_args()

    # Ch·∫°y th·ª±c t·∫ø l·∫•y tham s·ªë t·ª´ d√≤ng l·ªánh
    run_crawling(target_date=args.date, force_run=args.force)
    